{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3253e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import arxiv\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# The complete list of \"AI\" categories\n",
    "CATEGORIES = [\n",
    "    \"cs.AI\", \"cs.LG\", \"cs.CL\", \"cs.CV\", \n",
    "    \"cs.RO\", \"cs.MA\", \"stat.ML\", \"cs.NE\"\n",
    "]\n",
    "\n",
    "YEAR = 2025\n",
    "MONTH = 11\n",
    "DOWNLOAD_DIR = \"/media/mascit/datasets/arxiv/2025_1112\"\n",
    "SLEEP_TIME = 4 \n",
    "\n",
    "\n",
    "\n",
    "def download_pdf(paper, save_dir):\n",
    "    \"\"\"Downloads PDF with safe filename.\"\"\"\n",
    "    # Clean title\n",
    "    safe_title = \"\".join(x for x in paper.title if x.isalnum() or x in \" -_\").strip()\n",
    "    filename = f\"{paper.get_short_id()}_{safe_title[:50]}.pdf\"\n",
    "    file_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        return False  # Skip existing\n",
    "\n",
    "    # Use export.arxiv.org for scripts\n",
    "    pdf_url = paper.pdf_url.replace(\"arxiv.org\", \"export.arxiv.org\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(pdf_url, stream=True, headers={\"User-Agent\": \"AI_Researcher/1.0\"})\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            return True\n",
    "        elif response.status_code == 403:\n",
    "            print(\"\\n[!] 403 Forbidden. You might be blocked. Pause the script.\")\n",
    "            sys.exit(-1)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(DOWNLOAD_DIR):\n",
    "        os.makedirs(DOWNLOAD_DIR)\n",
    "\n",
    "    print(f\"Fetching metadata for AI categories: {CATEGORIES}\")\n",
    "    \n",
    "    # Construct query: (cat:cs.CV OR cat:cs.CL OR ...)\n",
    "    query = \" OR \".join([f\"cat:{cat}\" for cat in CATEGORIES])\n",
    "    \n",
    "    client = arxiv.Client(\n",
    "        page_size=1000,\n",
    "        delay_seconds=1,\n",
    "        num_retries=5\n",
    "    )\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    target_papers = []\n",
    "    \n",
    "    # --- Step 1: Filter Metadata ---\n",
    "    # We fetch ALL results and filter by year client-side because \n",
    "    # the API's date filtering is often unreliable.\n",
    "    print(\"Scanning metadata (this may take a few minutes)...\")\n",
    "    try:\n",
    "        for result in tqdm(client.results(search), desc=\"Fetching Metadata\"):\n",
    "            if result.published.year < YEAR or result.published.month < MONTH:\n",
    "                break # Stop once we hit 2024\n",
    "            \n",
    "            if result.published.year == YEAR and result.published.month >= MONTH:\n",
    "                target_papers.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Metadata fetch interrupted: {e}\")\n",
    "\n",
    "    # Remove duplicates (papers often listed in multiple categories)\n",
    "    # We use a dictionary keyed by entry_id to ensure uniqueness\n",
    "    unique_papers = {p.entry_id: p for p in target_papers}.values()\n",
    "    \n",
    "    print(f\"\\nFound {len(unique_papers)} unique AI papers from {YEAR}.\")\n",
    "    \n",
    "    # --- Step 2: Download ---\n",
    "    print(f\"Starting downloads to {DOWNLOAD_DIR}...\")\n",
    "    \n",
    "    for paper in tqdm(list(unique_papers), desc=\"Downloading PDFs\"):\n",
    "        success = download_pdf(paper, DOWNLOAD_DIR)\n",
    "        if success:\n",
    "            time.sleep(SLEEP_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2017e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metadata for AI categories: ['cs.AI', 'cs.LG', 'cs.CL', 'cs.CV', 'cs.RO', 'cs.MA', 'stat.ML', 'cs.NE']\n",
      "Scanning metadata (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Metadata: 10000it [01:15, 132.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata fetch interrupted: Page request resulted in HTTP 500 (https://export.arxiv.org/api/query?search_query=cat%3Acs.AI+OR+cat%3Acs.LG+OR+cat%3Acs.CL+OR+cat%3Acs.CV+OR+cat%3Acs.RO+OR+cat%3Acs.MA+OR+cat%3Astat.ML+OR+cat%3Acs.NE&id_list=&sortBy=submittedDate&sortOrder=descending&start=10000&max_results=1000)\n",
      "\n",
      "Found 10000 unique AI papers from 2025.\n",
      "Starting downloads to /media/mascit/datasets/arxiv/2025_1112...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading PDFs:   0%|          | 35/10000 [03:33<16:53:24,  6.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 100\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     98\u001b[39m success = download_pdf(paper, DOWNLOAD_DIR)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     time.sleep(SLEEP_TIME)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradientlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
