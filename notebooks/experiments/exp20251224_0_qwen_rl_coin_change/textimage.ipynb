{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b70551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e52b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import re\n",
    "\n",
    "class ImageTextGen:\n",
    "    def __init__(self, canvas_size=(128, 128), font_path=None, font_size=10, custom_space_width=None):\n",
    "        \"\"\"\n",
    "        Inizializza il generatore.\n",
    "\n",
    "        Args:\n",
    "            canvas_size (tuple): (width, height).\n",
    "            font_path (str): Path al file .ttf.\n",
    "            font_size (int): Dimensione font.\n",
    "            custom_space_width (int, optional):\n",
    "                - Se None: usa spaziatura standard del font.\n",
    "                - Se int: usa spaziatura custom per gli spazi (avanzamento fisso).\n",
    "        \"\"\"\n",
    "        self.width, self.height = canvas_size\n",
    "        self.font_size = font_size\n",
    "        self.custom_space_width = custom_space_width\n",
    "\n",
    "        if font_path and os.path.exists(font_path):\n",
    "            self.font = ImageFont.truetype(font_path, font_size)\n",
    "        else:\n",
    "            print(f\"Warning: Font '{font_path}' non trovato. Uso default.\")\n",
    "            self.font = ImageFont.load_default()\n",
    "\n",
    "    def _get_line_height(self):\n",
    "        \"\"\"Calcola altezza riga in modo sicuro per entrambi i metodi.\"\"\"\n",
    "        bbox = self.font.getbbox(\"Hg\")\n",
    "        return bbox[3] - bbox[1] + 1\n",
    "\n",
    "    def render(self, text):\n",
    "        \"\"\"Metodo principale che smista la logica.\"\"\"\n",
    "        if self.custom_space_width is not None:\n",
    "            return self._render_custom(text)\n",
    "        else:\n",
    "            return self._render_standard(text)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Helpers: char-level fitting\n",
    "    # ----------------------------\n",
    "    def _fit_prefix_len(self, s: str, max_w: float, measure_fn) -> int:\n",
    "        \"\"\"\n",
    "        Ritorna la lunghezza massima del prefisso di `s` che entra in `max_w`.\n",
    "        Usa binary search su measure_fn(s[:k]).\n",
    "        \"\"\"\n",
    "        if not s:\n",
    "            return 0\n",
    "\n",
    "        eps = 1e-6\n",
    "        # Se non entra nemmeno 1 char, forziamo 1 per evitare loop infinito\n",
    "        if measure_fn(s[:1]) > max_w + eps:\n",
    "            return 1\n",
    "\n",
    "        lo, hi = 1, len(s)\n",
    "        while lo < hi:\n",
    "            mid = (lo + hi + 1) // 2\n",
    "            if measure_fn(s[:mid]) <= max_w + eps:\n",
    "                lo = mid\n",
    "            else:\n",
    "                hi = mid - 1\n",
    "        return lo\n",
    "\n",
    "    # ==========================================\n",
    "    # LOGICA STANDARD (Veloce, Spaziatura Font)\n",
    "    # ==========================================\n",
    "    def _render_standard(self, text):\n",
    "        lines = self._wrap_text_standard_charwise(text)\n",
    "        line_height = self._get_line_height()\n",
    "\n",
    "        images = []\n",
    "        current_image = Image.new('L', (self.width, self.height), color=0)\n",
    "        draw = ImageDraw.Draw(current_image)\n",
    "        y_cursor = 0\n",
    "\n",
    "        for line in lines:\n",
    "            if y_cursor + line_height > self.height:\n",
    "                images.append(current_image)\n",
    "                current_image = Image.new('L', (self.width, self.height), color=0)\n",
    "                draw = ImageDraw.Draw(current_image)\n",
    "                y_cursor = 0\n",
    "\n",
    "            # line può essere \"\" (riga vuota): ok, avanza comunque\n",
    "            if line:\n",
    "                draw.text((0, y_cursor), line, font=self.font, fill=255)\n",
    "            y_cursor += line_height\n",
    "\n",
    "        images.append(current_image)\n",
    "        return images\n",
    "\n",
    "    def _wrap_text_standard_charwise(self, text: str):\n",
    "        \"\"\"\n",
    "        Wrap a livello carattere usando la misura reale del font.\n",
    "        Mantiene i newline espliciti.\n",
    "        \"\"\"\n",
    "        # Normalizza newline\n",
    "        text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "        lines = []\n",
    "        for paragraph in text.split(\"\\n\"):\n",
    "            # Preserva righe vuote\n",
    "            if paragraph == \"\":\n",
    "                lines.append(\"\")\n",
    "                continue\n",
    "\n",
    "            remaining = paragraph\n",
    "            while remaining:\n",
    "                if self.font.getlength(remaining) <= self.width:\n",
    "                    lines.append(remaining.rstrip())\n",
    "                    break\n",
    "\n",
    "                k = self._fit_prefix_len(remaining, self.width, self.font.getlength)\n",
    "                chunk = remaining[:k].rstrip()\n",
    "                lines.append(chunk)\n",
    "\n",
    "                remaining = remaining[k:]\n",
    "                # Evita che le righe wrappate inizino con spazi (tipico comportamento)\n",
    "                remaining = remaining.lstrip(\" \")\n",
    "\n",
    "        return lines\n",
    "\n",
    "    # ==========================================\n",
    "    # LOGICA CUSTOM (Precisa, Spaziatura Manuale)\n",
    "    # ==========================================\n",
    "    def _render_custom(self, text):\n",
    "        \"\"\"\n",
    "        Render custom: spazi con avanzamento fisso (custom_space_width),\n",
    "        testo misurato e disegnato in run (per performance/kerning interno alla run).\n",
    "        \"\"\"\n",
    "        lines_tokens = self._wrap_text_custom_charwise(text)\n",
    "        line_height = self._get_line_height()\n",
    "        space_w = float(self.custom_space_width)\n",
    "\n",
    "        images = []\n",
    "        current_image = Image.new('L', (self.width, self.height), color=0)\n",
    "        draw = ImageDraw.Draw(current_image)\n",
    "        y_cursor = 0\n",
    "\n",
    "        for tokens in lines_tokens:\n",
    "            if y_cursor + line_height > self.height:\n",
    "                images.append(current_image)\n",
    "                current_image = Image.new('L', (self.width, self.height), color=0)\n",
    "                draw = ImageDraw.Draw(current_image)\n",
    "                y_cursor = 0\n",
    "\n",
    "            x_cursor = 0.0\n",
    "            for tok in tokens:\n",
    "                if tok == \" \":\n",
    "                    x_cursor += space_w\n",
    "                else:\n",
    "                    draw.text((x_cursor, y_cursor), tok, font=self.font, fill=255)\n",
    "                    x_cursor += self.font.getlength(tok)\n",
    "\n",
    "            y_cursor += line_height\n",
    "\n",
    "        images.append(current_image)\n",
    "        return images\n",
    "\n",
    "    def _wrap_text_custom_charwise(self, text: str):\n",
    "        \"\"\"\n",
    "        Wrap a livello carattere con gestione spazi a larghezza fissa.\n",
    "        - Preserva newline espliciti\n",
    "        - Preserva spazi multipli (ma non li mette a inizio riga dopo wrap)\n",
    "        - Spezza anche parole lunghissime a livello char\n",
    "        \"\"\"\n",
    "        text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        # Tratta i tab come 4 spazi (puoi cambiare se ti serve)\n",
    "        text = text.replace(\"\\t\", \"    \")\n",
    "\n",
    "        space_w = float(self.custom_space_width)\n",
    "        lines = []\n",
    "\n",
    "        for paragraph in text.split(\"\\n\"):\n",
    "            if paragraph == \"\":\n",
    "                lines.append([])  # riga vuota\n",
    "                continue\n",
    "\n",
    "            # Runs: o sequenze di spazi, o sequenze di non-spazi\n",
    "            runs = re.findall(r\" +|[^ ]+\", paragraph)\n",
    "\n",
    "            current_tokens = []\n",
    "            current_w = 0.0\n",
    "\n",
    "            def flush_line():\n",
    "                nonlocal current_tokens, current_w\n",
    "                lines.append(current_tokens)\n",
    "                current_tokens = []\n",
    "                current_w = 0.0\n",
    "\n",
    "            for run in runs:\n",
    "                if not run:\n",
    "                    continue\n",
    "\n",
    "                if run[0] == \" \":\n",
    "                    # Spazi: aggiungi char per char (larghezza fissa)\n",
    "                    for _ in run:\n",
    "                        # Evita spazi a inizio riga dopo un wrap\n",
    "                        if not current_tokens:\n",
    "                            continue\n",
    "                        if current_w + space_w <= self.width:\n",
    "                            current_tokens.append(\" \")\n",
    "                            current_w += space_w\n",
    "                        else:\n",
    "                            flush_line()\n",
    "                            # dopo wrap non aggiungiamo spazi iniziali\n",
    "                    continue\n",
    "\n",
    "                # Run di testo (senza spazi): può richiedere split charwise\n",
    "                remaining = run\n",
    "                while remaining:\n",
    "                    # Se la riga è piena, vai a capo\n",
    "                    if current_tokens and current_w >= self.width:\n",
    "                        flush_line()\n",
    "\n",
    "                    remaining_w = self.width - current_w\n",
    "                    run_w = self.font.getlength(remaining)\n",
    "\n",
    "                    if run_w <= remaining_w:\n",
    "                        current_tokens.append(remaining)\n",
    "                        current_w += run_w\n",
    "                        remaining = \"\"\n",
    "                    else:\n",
    "                        k = self._fit_prefix_len(remaining, remaining_w, self.font.getlength)\n",
    "                        part = remaining[:k]\n",
    "                        part_w = self.font.getlength(part)\n",
    "\n",
    "                        # Safety: se non entra nulla (caso limite), forza 1 char\n",
    "                        if k <= 0:\n",
    "                            part = remaining[:1]\n",
    "                            part_w = self.font.getlength(part)\n",
    "                            remaining = remaining[1:]\n",
    "                        else:\n",
    "                            remaining = remaining[k:]\n",
    "\n",
    "                        current_tokens.append(part)\n",
    "                        current_w += part_w\n",
    "                        flush_line()\n",
    "\n",
    "            # Chiudi l'ultima linea del paragraph (anche se vuota)\n",
    "            lines.append(current_tokens)\n",
    "\n",
    "        return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1773f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"\n",
    "# TOC\n",
    "\n",
    "Il paper **4D-RGPT (arxiv:2512.17012)** pubblicato da NVIDIA affronta i limiti dei modelli multimodali attuali (MLLM) nella percezione dello spazio 4D (3D + tempo). Sebbene i modelli esistenti eccellano nel vision-language, faticano a comprendere la profondità e la dinamica temporale. 4D-RGPT introduce un framework di **Perceptual 4D Distillation (P4D)** che trasferisce la conoscenza da un modello esperto 4D \"frozen\" a un LLM student senza aggiungere costi computazionali in fase di inferenza.\n",
    "\n",
    "L'architettura utilizza un encoder regionale dedicato e un sistema di encoding temporale migliorato per rispondere a domande complesse su oggetti in movimento. Il lavoro introduce anche **R4D-Bench**, un benchmark specifico per il ragionamento 4D a livello di regione, colmando il gap tra la percezione visiva 2D e la comprensione geometrica tridimensionale nel tempo.\n",
    "\n",
    "- Code: /\n",
    "- Paper: [https://arxiv.org/pdf/2512.17012](https://arxiv.org/pdf/2512.17012)\n",
    "- Project Page: [https://ca-joe-yang.github.io/resource/projects/4D_RGPT](https://ca-joe-yang.github.io/resource/projects/4D_RGPT)\n",
    "- HuggingFace: [https://huggingface.co/papers/2512.17012](https://huggingface.co/papers/2512.17012)\n",
    "\n",
    "## 4D-RGPT Domande di ricerca\n",
    "\n",
    "- **Come integrare la percezione 4D a grana fine nei MLLM senza introdurre overhead in fase di inferenza?**\n",
    "Il paper propone la **Perceptual 4D Distillation (P4D)**. Questa tecnica permette di addestrare lo student MLLM a imitare le rappresentazioni di un encoder esperto 4D. Poiché i moduli di distillazione sono utilizzati solo durante il training e rimossi in fase di produzione, il modello mantiene la velocità di un LLM standard pur possedendo capacità percettive avanzate.\n",
    "- **In che modo è possibile migliorare la consapevolezza temporale dei token visivi per task di tracking e profondità?**\n",
    "Gli autori introducono il **Timestamp Positional Encoding (TPE)**. A differenza dei positional encodings standard, il TPE inietta segnali temporali espliciti direttamente nelle feature estratte dall'encoder regionale. Questo permette al trasformatore di distinguere la progressione cronologica degli oggetti all'interno di una sequenza video complessa.\n",
    "- **Quali sono le lacune dei benchmark attuali nella valutazione del ragionamento 4D?**\n",
    "I benchmark esistenti si concentrano spesso su scene statiche o descrizioni globali del video. Il paper dimostra che questi non misurano la capacità di \"grounding\" spaziale. Per questo viene creato **R4D-Bench**, che richiede al modello di identificare regioni specifiche e rispondere a domande sulla loro evoluzione geometrica e interazione nel tempo.\n",
    "\n",
    "## 4D-RGPT Metodi e Tecniche\n",
    "\n",
    "L'architettura di 4D-RGPT si basa su tre pilastri fondamentali che permettono la transizione da una comprensione puramente visiva a una percezione spaziale dinamica.\n",
    "\n",
    "### Perceptual 4D Distillation (P4D)\n",
    "\n",
    "La tecnica core è la distillazione della percezione 4D. Viene utilizzato un modello insegnante (teacher) specializzato in point cloud o video 4D. Il processo di ottimizzazione minimizza la discrepanza tra le feature dello student e quelle del teacher attraverso una loss combinata:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{distill} = \\lambda_{latent} \\mathcal{L}_{latent} + \\lambda_{explicit} \\mathcal{L}_{explicit}\n",
    "$$\n",
    "\n",
    "La componente $ \\mathcal{L}_{latent} $ agisce nello spazio delle feature latenti, calcolando la distanza tra le mappe di attivazione dell'encoder dello student e del teacher:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{latent} = \\sum_{i} \\| \\phi_{s}(x_i) - \\text{proj}(\\phi_{t}(x_i)) \\|_2^2\n",
    "$$\n",
    "\n",
    "### 4D Regional Encoder e TPE\n",
    "\n",
    "Per gestire il \"region-level understanding\", il modello utilizza un **4D Regional Encoder**. Questo modulo estrae token specifici per le aree di interesse segnate tramite bounding box. Il **Timestamp Positional Encoding (TPE)** viene aggiunto per preservare l'ordine temporale dei frame:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimestampPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(max_len, d_model))\n",
    "        \n",
    "    def forward(self, regional_features, timestamps):\n",
    "        # regional_features: [batch, frames, regions, dim]\n",
    "        # timestamps: indici temporali dei frame\n",
    "        t_embed = self.encoding[timestamps, :]\n",
    "        return regional_features + t_embed.unsqueeze(2)\n",
    "```\n",
    "\n",
    "### Explicit Distillation Task\n",
    "\n",
    "Oltre alla loss latente, il modello viene addestrato su task espliciti di ricostruzione 4D, come la predizione del flusso ottico (optical flow) o della profondità relativa. Questo forza lo student a catturare segnali geometrici reali piuttosto che limitarsi a correlazioni testuali.\n",
    "\n",
    "## 4D-RGPT Dataset\n",
    "\n",
    "Il contributo principale a livello di dati è **R4D-Bench**. Si tratta di un benchmark per la Region-level 4D Video Question Answering, costruito con una pipeline ibrida automatizzata e verificata da esseri umani. Include scene dinamiche con variazioni di profondità e interazioni tra oggetti.\n",
    "\n",
    "Per l'addestramento, 4D-RGPT utilizza anche dataset 3D/4D esistenti come **ScanNet** e **Waymo Open Dataset**, riconvertiti per task di istruzione multimodale.\n",
    "\n",
    "## 4D-RGPT Licenze\n",
    "\n",
    "Il dataset R4D-Bench e il codice di 4D-RGPT non hanno una licenza esplicitamente dichiarata nel pre-print.\n",
    "\n",
    "# FAQ - FAQ per 4D-RGPT\n",
    "\n",
    "## In cosa differisce 4D-RGPT da un normale modello Video-LLM?\n",
    "\n",
    "La maggior parte dei Video-LLM tratta i video come una sequenza di immagini 2D. 4D-RGPT, invece, \"vede\" la profondità e la struttura geometrica degli oggetti grazie alla distillazione da modelli esperti di point cloud, permettendo un ragionamento spaziale molto più preciso.\n",
    "\n",
    "## La Perceptual Distillation rallenta l'inferenza del modello?\n",
    "\n",
    "No. Uno dei vantaggi chiave è che i moduli del modello \"teacher\" e le teste di distillazione vengono utilizzate esclusivamente durante la fase di training (training-only). Una volta addestrato, il modello student (4D-RGPT) opera in autonomia senza costi aggiuntivi.\n",
    "\n",
    "## Quali sono le applicazioni pratiche di questa tecnologia?\n",
    "\n",
    "4D-RGPT è ideale per ambiti dove la precisione spaziale è critica, come la guida autonoma (per capire la traiettoria di altri veicoli), la robotica collaborativa e l'ispezione industriale assistita da AI, dove è necessario identificare anomalie in oggetti che si muovono nello spazio 3D.\n",
    "\n",
    "# END_FAQ\n",
    "\"\"\".replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 11 images.\n"
     ]
    }
   ],
   "source": [
    "# long_text = \"In **Step-DeepResearch Technical Report** (arxiv:2512.20491) il team di StepFun presenta un modello da 32 miliardi di parametri progettato per compiti di \\\"Deep Research\\\" (ricerca approfondita a lungo orizzonte). L'approccio sfida il fatto che siano necessari modelli enormi per la ricerca complessa. Il core del lavoro è una strategia di sintesi dei dati basata su **Atomic Capabilities** (capacità atomiche) come pianificazione, raccolta informazioni e scrittura di report. Invece di addestrare il modello su intere traiettorie rumorose, gli autori scompongono il processo in skill fondamentali, addestrate progressivamente tramite un percorso che va dal *mid-training agentico*, al *Supervised Fine-Tuning (SFT)* fino al *Reinforcement Learning (RL)*. Il modello utilizza un'architettura a singolo agente in stile ReAct e viene valutato su un nuovo benchmark, **ADR-Bench**, dimostrando efficienza e qualità paragonabili a modelli closed-source molto più grandi.\"\n",
    "\n",
    "gen = ImageTextGen(\n",
    "    canvas_size=(128, 128), \n",
    "    font_path=\"/Users/mascit/Downloads/CozetteVector.ttf\", #\"/Users/mascit/Downloads/unifont-17.0.03.otf\", #\"/Users/mascit/Downloads/CozetteVector.ttf\", #\"/Users/mascit/Downloads/04b_03/04B_03__.TTF\",\n",
    "    font_size=8,\n",
    "    custom_space_width=3\n",
    ")\n",
    "\n",
    "# Render\n",
    "result_images = gen.render(long_text)\n",
    "\n",
    "print(f\"Generated {len(result_images)} images.\")\n",
    "\n",
    "# Save for inspection\n",
    "for i, img in enumerate(result_images):\n",
    "    img.save(f\"output_page_{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "247bc6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "725431a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128 / 8) **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "656a6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "arr = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5779e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56677ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d7b2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1422.45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t) / 400 * 90 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradientlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
